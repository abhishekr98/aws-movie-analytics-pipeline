{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Movie Analytics Pipeline\n",
    "\n",
    "End-to-end workflow demonstration using AWS Glue, Athena, and S3.\n",
    "\n",
    "## Steps Covered\n",
    "1. **Data Ingestion**: Uploaded raw movie dataset files (`credits.csv`, `keywords.csv`, etc.) to S3 bucket `movieanalysis25/raw/`.\n",
    "2. **Flattening**: Used AWS Glue PySpark scripts to parse and flatten JSON columns (e.g., `cast`, `crew`, `keywords`).\n",
    "3. **Storage**: Saved flattened data as CSVs in S3 bucket `movieanalysis25/cleaned/` for downstream analytics.\n",
    "4. **Athena Integration**: Created external tables in Athena to query the cleaned datasets.\n",
    "5. **Analysis**: Ran SQL queries to extract insights from the transformed datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Glue job parameters\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Load raw credits CSV from S3\n",
    "df = glueContext.create_dynamic_frame.from_options(\n",
    "    format_options={\"quoteChar\": '\"', \"withHeader\": True, \"separator\": \",\"},\n",
    "    connection_type=\"s3\",\n",
    "    format=\"csv\",\n",
    "    connection_options={\"paths\": [\"s3://movieanalysis25/raw/credits.csv\"], \"recurse\": True}\n",
    ").toDF()\n",
    "\n",
    "# Flatten 'cast' JSON array\n",
    "df = df.withColumn(\"cast_exploded\", F.explode(F.from_json(F.col(\"cast\"), \"array<struct<cast_id:int,character:string,credit_id:string,gender:int,id:int,name:string,order:int>>\")))\n",
    "df_flat = df.select(\n",
    "    \"id\",\n",
    "    F.col(\"cast_exploded.cast_id\").alias(\"cast_id\"),\n",
    "    F.col(\"cast_exploded.character\").alias(\"character\"),\n",
    "    F.col(\"cast_exploded.credit_id\").alias(\"credit_id\"),\n",
    "    F.col(\"cast_exploded.gender\").alias(\"gender\"),\n",
    "    F.col(\"cast_exploded.name\").alias(\"name\"),\n",
    "    F.col(\"cast_exploded.order\").alias(\"order_num\")\n",
    ")\n",
    "\n",
    "# Write flattened CSV back to S3\n",
    "df_flat.write.mode(\"overwrite\").option(\"header\", True).csv(\"s3://movieanalysis25/cleaned/credits/cast_csv/\")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Athena Table Creation\n",
    "\n",
    "```sql\n",
    "CREATE EXTERNAL TABLE movie_analysis.cast_data (\n",
    "    id STRING,\n",
    "    cast_id STRING,\n",
    "    character STRING,\n",
    "    credit_id STRING,\n",
    "    gender INT,\n",
    "    name STRING,\n",
    "    order_num INT\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "WITH SERDEPROPERTIES (\n",
    "  \"separatorChar\" = \",\",\n",
    "  \"quoteChar\"     = \"\\\"\"\n",
    ")\n",
    "LOCATION 's3://movieanalysis25/cleaned/credits/cast_csv/'\n",
    "TBLPROPERTIES ('skip.header.line.count'='1');\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Athena Query\n",
    "```sql\n",
    "SELECT name, COUNT(*) AS appearances\n",
    "FROM movie_analysis.cast_data\n",
    "GROUP BY name\n",
    "ORDER BY appearances DESC\n",
    "LIMIT 10;\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
